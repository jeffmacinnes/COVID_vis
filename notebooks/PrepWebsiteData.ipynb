{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep Website Data\n",
    "\n",
    "Prep datafiles for use in specific sections of the website\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydeck as pdk\n",
    "import os\n",
    "from os.path import join\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir = '../data'\n",
    "outputDir = '../websiteData'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "arts = []\n",
    "with open(join(dataDir, 'articleMetadata.json')) as f:\n",
    "    for i,line in enumerate(f):\n",
    "        thisArt = json.loads(line)\n",
    "        if 'isValid' in thisArt.keys():\n",
    "            arts.append(thisArt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hero\n",
    "\n",
    "Create a sample of article titles used to drive the hero animation. This sample should be manually spot checked afterward to prune out titles that are less obviously related to the current pandemic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a random sample of journal articles\n",
    "random.seed(101101)\n",
    "sampleArts = random.sample(arts, 1000)\n",
    "\n",
    "# extract the relevant metadata (title, journal name, etc) for each\n",
    "# and save to csv\n",
    "heroData = []\n",
    "for a in sampleArts:\n",
    "    title = a['title']\n",
    "    if title:\n",
    "        title = title.replace('\\n', '').strip()\n",
    "        \n",
    "    journal = a['journalTitle']\n",
    "    if journal:\n",
    "        journal = journal.replace('\\n', '').strip()\n",
    "    \n",
    "    heroData.append({\n",
    "        'title': title,\n",
    "        'journal': journal,\n",
    "        'pubDate': a['pubDate']\n",
    "    })\n",
    "\n",
    "heroData_df = pd.DataFrame(heroData)\n",
    "heroData_df.to_csv(join(outputDir, 'heroData.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heroData_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Papers per year\n",
    "\n",
    "For the visualization showing the number of coronavirus related papers from the late 60's on to 2020. \n",
    "\n",
    "No processing steps required here, just a note about how that data was acquired: \n",
    "\n",
    "The `papersPerYear.csv` was obtained by going to pubmed central and using the covid-19 search string, without the publication date range:\n",
    "\n",
    "```\n",
    "\"COVID-19\"[All Fields] OR \"COVID-19\"[MeSH Terms] OR \"COVID-19 Vaccines\"[All Fields] OR \"COVID-19 Vaccines\"[MeSH Terms] OR \"COVID-19 serotherapy\"[All Fields] OR \"COVID-19 Nucleic Acid Testing\"[All Fields] OR \"covid-19 nucleic acid testing\"[MeSH Terms] OR \"COVID-19 Serological Testing\"[All Fields] OR \"covid-19 serological testing\"[MeSH Terms] OR \"COVID-19 Testing\"[All Fields] OR \"covid-19 testing\"[MeSH Terms] OR \"SARS-CoV-2\"[All Fields] OR \"sars-cov-2\"[MeSH Terms] OR \"Severe Acute Respiratory Syndrome Coronavirus 2\"[All Fields] OR \"NCOV\"[All Fields] OR \"2019 NCOV\"[All Fields] OR ((\"coronavirus\"[MeSH Terms] OR \"coronavirus\"[All Fields] OR \"COV\"[All Fields]) ) \n",
    "```\n",
    "\n",
    "Next, using the returned results, I applied a custom filter (using the left hand panel) to define a date range for each year from 1965 onwards. For instance, 1965 was setting the custom date range to `1965/01/01` to `1965/12/31`. \n",
    "\n",
    "I manually recorded the number of articles returned by this filter. Repeated for every year thereafter. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaboration map\n",
    "\n",
    "In order to store this data as efficiently as possible, create separate datasets for:\n",
    "\n",
    "* a main geoIDs dataset. One row per unique geoID, stores all of the relevant info (addr, state, country, lat, lng) for each geoID. This dataset should be easily indexable\n",
    "\n",
    "* the collaborations. For each collaboration, we need to know (at minimum) the date, src geoID idx, dst geoID idx\n",
    "\n",
    "* total article states by date. For each date throughout 2020, store the number of (authors? papers? collaborations?) accumulated by that date\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create the main geoIDs dataset\n",
    "cityGeocodes_df = pd.read_csv(join(dataDir, 'cityGeocodes.csv'))\n",
    "\n",
    "# add idx column\n",
    "cityGeocodes_df['idx'] = cityGeocodes_df.index\n",
    "\n",
    "# save a version for site, removing all unecessary columns\n",
    "siteGeoIDs = cityGeocodes_df[['idx', 'lat', 'lng']]\n",
    "\n",
    "# write to web folder\n",
    "siteGeoIDs.to_csv(join(outputDir, 'geoIDs.csv'), float_format='%.2f', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGeoIdx(geoID):\n",
    "    \"\"\" return the index of the specified geoID from the cityGeoCodes_df dataframe \"\"\"\n",
    "    indices = cityGeocodes_df.index[cityGeocodes_df['geoID'] == geoID].tolist()\n",
    "    return indices[0]  # should only ever be 1 value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step takes ~10min to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGeoIdx(geoID):\n",
    "    \"\"\" return the index of the specified geoID from the cityGeoCodes_df dataframe \"\"\"\n",
    "    indices = cityGeocodes_df.index[cityGeocodes_df['geoID'] == geoID].tolist()\n",
    "    return indices[0]  # should only ever be 1 value\n",
    "\n",
    "\n",
    "### create the collaborations dataset\n",
    "collabs_df = pd.read_csv(join(dataDir, 'processed/collaborations.csv'))\n",
    "\n",
    "# add the srcIdx and dstIdx cols to each row\n",
    "collabs_df['srcIdx'] = collabs_df['geoID_A'].apply(getGeoIdx)\n",
    "collabs_df['dstIdx'] = collabs_df['geoID_B'].apply(getGeoIdx)\n",
    "\n",
    "# drop missing rows\n",
    "collabs_df.dropna(inplace=True)\n",
    "\n",
    "# drop the year from pub date to save space\n",
    "collabs_df['pubDate'] = collabs_df['pubDate'].apply(lambda x: '-'.join(x.split('-')[1:]))\n",
    "\n",
    "# save a version for site, removing unecessary columns\n",
    "siteCollabs = collabs_df[['pubDate', 'srcIdx', 'dstIdx']].sort_values('pubDate')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write data to file\n",
    "siteCollabs.to_csv(join(outputDir, 'collabsByDate.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'01-01'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'-'.join(d.split('-')[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8039]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cityGeocodes_df.index[cityGeocodes_df['geoID'] == 'ChIJU3OqqNAywkcRUG1NL6uZAAQ'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
