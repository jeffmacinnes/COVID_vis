{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep Website Data\n",
    "\n",
    "Prep datafiles for use in specific sections of the website\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydeck as pdk\n",
    "import os\n",
    "from os.path import join\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir = '../data'\n",
    "outputDir = '../websiteData'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "arts = []\n",
    "with open(join(dataDir, 'articleMetadata.json')) as f:\n",
    "    for i,line in enumerate(f):\n",
    "        thisArt = json.loads(line)\n",
    "        if 'isValid' in thisArt.keys():\n",
    "            arts.append(thisArt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hero\n",
    "\n",
    "Create a sample of article titles used to drive the hero animation. This sample should be manually spot checked afterward to prune out titles that are less obviously related to the current pandemic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a random sample of journal articles\n",
    "random.seed(101101)\n",
    "sampleArts = random.sample(arts, 1000)\n",
    "\n",
    "# extract the relevant metadata (title, journal name, etc) for each\n",
    "# and save to csv\n",
    "heroData = []\n",
    "for a in sampleArts:\n",
    "    title = a['title']\n",
    "    if title:\n",
    "        title = title.replace('\\n', '').strip()\n",
    "        \n",
    "    journal = a['journalTitle']\n",
    "    if journal:\n",
    "        journal = journal.replace('\\n', '').strip()\n",
    "    \n",
    "    heroData.append({\n",
    "        'title': title,\n",
    "        'journal': journal,\n",
    "        'pubDate': a['pubDate']\n",
    "    })\n",
    "\n",
    "heroData_df = pd.DataFrame(heroData)\n",
    "heroData_df.to_csv(join(outputDir, 'heroData.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heroData_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Papers per year\n",
    "\n",
    "For the visualization showing the number of coronavirus related papers from the late 60's on to 2020. \n",
    "\n",
    "No processing steps required here, just a note about how that data was acquired: \n",
    "\n",
    "The `papersPerYear.csv` was obtained by going to pubmed central and using the covid-19 search string, without the publication date range:\n",
    "\n",
    "```\n",
    "\"COVID-19\"[All Fields] OR \"COVID-19\"[MeSH Terms] OR \"COVID-19 Vaccines\"[All Fields] OR \"COVID-19 Vaccines\"[MeSH Terms] OR \"COVID-19 serotherapy\"[All Fields] OR \"COVID-19 Nucleic Acid Testing\"[All Fields] OR \"covid-19 nucleic acid testing\"[MeSH Terms] OR \"COVID-19 Serological Testing\"[All Fields] OR \"covid-19 serological testing\"[MeSH Terms] OR \"COVID-19 Testing\"[All Fields] OR \"covid-19 testing\"[MeSH Terms] OR \"SARS-CoV-2\"[All Fields] OR \"sars-cov-2\"[MeSH Terms] OR \"Severe Acute Respiratory Syndrome Coronavirus 2\"[All Fields] OR \"NCOV\"[All Fields] OR \"2019 NCOV\"[All Fields] OR ((\"coronavirus\"[MeSH Terms] OR \"coronavirus\"[All Fields] OR \"COV\"[All Fields]) ) \n",
    "```\n",
    "\n",
    "Next, using the returned results, I applied a custom filter (using the left hand panel) to define a date range for each year from 1965 onwards. For instance, 1965 was setting the custom date range to `1965/01/01` to `1965/12/31`. \n",
    "\n",
    "I manually recorded the number of articles returned by this filter. Repeated for every year thereafter. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaboration map\n",
    "\n",
    "In order to store this data as efficiently as possible, create separate datasets for:\n",
    "\n",
    "* a main geoIDs dataset. One row per unique geoID, stores all of the relevant info (addr, state, country, lat, lng) for each geoID. This dataset should be easily indexable\n",
    "\n",
    "* the collaborations. For each collaboration, we need to know (at minimum) the date, src geoID idx, dst geoID idx\n",
    "\n",
    "* total article states by date. For each date throughout 2020, store the number of (authors? papers? collaborations?) accumulated by that date\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create the main geoIDs dataset\n",
    "cityGeocodes_df = pd.read_csv(join(dataDir, 'cityGeocodes.csv'))\n",
    "\n",
    "# add idx column\n",
    "cityGeocodes_df['idx'] = cityGeocodes_df.index\n",
    "\n",
    "# save a version for site, removing all unecessary columns\n",
    "siteGeoIDs = cityGeocodes_df[['idx', 'lat', 'lng']]\n",
    "\n",
    "# write to web folder\n",
    "siteGeoIDs.to_csv(join(outputDir, 'geoIDs.csv'), float_format='%.2f', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGeoIdx(geoID):\n",
    "    \"\"\" return the index of the specified geoID from the cityGeoCodes_df dataframe \"\"\"\n",
    "    indices = cityGeocodes_df.index[cityGeocodes_df['geoID'] == geoID].tolist()\n",
    "    return indices[0]  # should only ever be 1 value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step takes ~10min to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGeoIdx(geoID):\n",
    "    \"\"\" return the index of the specified geoID from the cityGeoCodes_df dataframe \"\"\"\n",
    "    indices = cityGeocodes_df.index[cityGeocodes_df['geoID'] == geoID].tolist()\n",
    "    return indices[0]  # should only ever be 1 value\n",
    "\n",
    "\n",
    "### create the collaborations dataset\n",
    "collabs_df = pd.read_csv(join(dataDir, 'processed/collaborations.csv'))\n",
    "\n",
    "# add the srcIdx and dstIdx cols to each row\n",
    "collabs_df['srcIdx'] = collabs_df['geoID_A'].apply(getGeoIdx)\n",
    "collabs_df['dstIdx'] = collabs_df['geoID_B'].apply(getGeoIdx)\n",
    "\n",
    "# save a version for site, removing unecessary columns\n",
    "siteCollabs = collabs_df[['pubDate', 'srcIdx', 'dstIdx']].sort_values('pubDate')\n",
    "\n",
    "# write to web folder\n",
    "siteCollabs.to_csv(join(outputDir, 'collabsByDate.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PMCID</th>\n",
       "      <th>pubDate</th>\n",
       "      <th>geoID_A</th>\n",
       "      <th>geoID_B</th>\n",
       "      <th>lat_A</th>\n",
       "      <th>lng_A</th>\n",
       "      <th>fmtAddr_A</th>\n",
       "      <th>lat_B</th>\n",
       "      <th>lng_B</th>\n",
       "      <th>fmtAddr_B</th>\n",
       "      <th>srcIdx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PMC7581440</td>\n",
       "      <td>2020-10-23</td>\n",
       "      <td>ChIJ7faBuL7or0cRYDuslG2sJQQ</td>\n",
       "      <td>ChIJAVkDPzdOqEcRcDteW0YgIQQ</td>\n",
       "      <td>52.480909</td>\n",
       "      <td>10.550783</td>\n",
       "      <td>38518 Gifhorn, Germany</td>\n",
       "      <td>52.520007</td>\n",
       "      <td>13.404954</td>\n",
       "      <td>Berlin, Germany</td>\n",
       "      <td>7036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PMC7581502</td>\n",
       "      <td>2020-10-23</td>\n",
       "      <td>ChIJc8r44c9unUcRDZsdKH0cIJ0</td>\n",
       "      <td>ChIJuRMYfoNhsUcRoDrWe_I9JgQ</td>\n",
       "      <td>47.269212</td>\n",
       "      <td>11.404102</td>\n",
       "      <td>Innsbruck, Austria</td>\n",
       "      <td>53.551085</td>\n",
       "      <td>9.993682</td>\n",
       "      <td>Hamburg, Germany</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PMC7581692</td>\n",
       "      <td>2020-10-23</td>\n",
       "      <td>ChIJmb1k2ko-eUgRqdwTAv26rVE</td>\n",
       "      <td>ChIJra6o8IHuBUgRMO0NHlI3DQQ</td>\n",
       "      <td>53.800755</td>\n",
       "      <td>-1.549077</td>\n",
       "      <td>Leeds, UK</td>\n",
       "      <td>47.218371</td>\n",
       "      <td>-1.553621</td>\n",
       "      <td>Nantes, France</td>\n",
       "      <td>324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PMC7581692</td>\n",
       "      <td>2020-10-23</td>\n",
       "      <td>ChIJmb1k2ko-eUgRqdwTAv26rVE</td>\n",
       "      <td>ChIJsZ3dJQevthIRAuiUKHRWh60</td>\n",
       "      <td>53.800755</td>\n",
       "      <td>-1.549077</td>\n",
       "      <td>Leeds, UK</td>\n",
       "      <td>43.610769</td>\n",
       "      <td>3.876716</td>\n",
       "      <td>Montpellier, France</td>\n",
       "      <td>324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PMC7581692</td>\n",
       "      <td>2020-10-23</td>\n",
       "      <td>ChIJmb1k2ko-eUgRqdwTAv26rVE</td>\n",
       "      <td>ChIJt2BwZIrfekgRAW4XP28E3EI</td>\n",
       "      <td>53.800755</td>\n",
       "      <td>-1.549077</td>\n",
       "      <td>Leeds, UK</td>\n",
       "      <td>53.408371</td>\n",
       "      <td>-2.991573</td>\n",
       "      <td>Liverpool, UK</td>\n",
       "      <td>324</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        PMCID     pubDate                      geoID_A  \\\n",
       "0  PMC7581440  2020-10-23  ChIJ7faBuL7or0cRYDuslG2sJQQ   \n",
       "1  PMC7581502  2020-10-23  ChIJc8r44c9unUcRDZsdKH0cIJ0   \n",
       "2  PMC7581692  2020-10-23  ChIJmb1k2ko-eUgRqdwTAv26rVE   \n",
       "3  PMC7581692  2020-10-23  ChIJmb1k2ko-eUgRqdwTAv26rVE   \n",
       "4  PMC7581692  2020-10-23  ChIJmb1k2ko-eUgRqdwTAv26rVE   \n",
       "\n",
       "                       geoID_B      lat_A      lng_A               fmtAddr_A  \\\n",
       "0  ChIJAVkDPzdOqEcRcDteW0YgIQQ  52.480909  10.550783  38518 Gifhorn, Germany   \n",
       "1  ChIJuRMYfoNhsUcRoDrWe_I9JgQ  47.269212  11.404102      Innsbruck, Austria   \n",
       "2  ChIJra6o8IHuBUgRMO0NHlI3DQQ  53.800755  -1.549077               Leeds, UK   \n",
       "3  ChIJsZ3dJQevthIRAuiUKHRWh60  53.800755  -1.549077               Leeds, UK   \n",
       "4  ChIJt2BwZIrfekgRAW4XP28E3EI  53.800755  -1.549077               Leeds, UK   \n",
       "\n",
       "       lat_B      lng_B            fmtAddr_B  srcIdx  \n",
       "0  52.520007  13.404954      Berlin, Germany    7036  \n",
       "1  53.551085   9.993682     Hamburg, Germany     177  \n",
       "2  47.218371  -1.553621       Nantes, France     324  \n",
       "3  43.610769   3.876716  Montpellier, France     324  \n",
       "4  53.408371  -2.991573        Liverpool, UK     324  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collabs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8039]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cityGeocodes_df.index[cityGeocodes_df['geoID'] == 'ChIJU3OqqNAywkcRUG1NL6uZAAQ'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
