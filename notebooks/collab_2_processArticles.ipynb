{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from os.path import join\n",
    "from os.path import exists\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "import json\n",
    "import requests\n",
    "import xmltodict\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir = '../data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the publication dates database for all articles\n",
    "\n",
    "The publication date data for each article was obtained from a separate API than the article metadata. Process the publication date data to produce a clean table of publication dates by pmc ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pubDates_db = []\n",
    "with open(join(dataDir, 'articlePubDates.json')) as f:\n",
    "    for line in f:\n",
    "        pubDates_db.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pubDate(ID):\n",
    "    pubDateRecord = next((x for x in pubDates_db if x['PMCID'] == ID), None)\n",
    "    if not pubDateRecord:\n",
    "        return None\n",
    "    try:\n",
    "        # get the 'pubdate' field from the date types of this entry\n",
    "        pubDates = pubDateRecord['pubDates']\n",
    "        pubDateStr = next(x for x in pubDates if 'pubdate' in x.keys())['pubdate']\n",
    "\n",
    "        # parse the pubdate string, create datetime object for it\n",
    "        dateParts = pubDateStr.split(' ')\n",
    "        if len(dateParts) == 3:   # YYYY Mon DD\n",
    "            pub_dt = datetime.strptime(pubDateStr, '%Y %b %d')\n",
    "        elif len(dateParts) == 2: # YYYY Mon\n",
    "            # in rare cases, the date is specified like 2020 Mar-Sep. Convert to: 2020 Mar\n",
    "            if '-' in dateParts[1]:\n",
    "                pubDateStr = f\"{dateParts[0]} {dateParts[1].split('-')[0]}\"\n",
    "                \n",
    "            # other times, the date is like 2020 Autumn. Ignore these, as too imprecise\n",
    "            if dateParts[1] in ['Winter', 'Spring', 'Summer', 'Autumn', 'Fall']:\n",
    "                return \n",
    "            \n",
    "            pub_dt = datetime.strptime(pubDateStr, '%Y %b')\n",
    "        elif len(dateParts) == 1:  # YYYY\n",
    "            # too imprecise, return None\n",
    "            return None\n",
    "        \n",
    "        # standardize the date string (YYYY-MM-DD) and return\n",
    "        return pub_dt.strftime('%Y-%m-%d')\n",
    "        \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'Error: {e} on pubDateRecord:')\n",
    "        print(pubDateRecord)\n",
    "        return None\n",
    "        \n",
    "    return pubDateStr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a clean table of publication dates for everything in the pubDates database (this step takes ~10 mins to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96998"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pubDates_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 96998 dates\n",
      "0, 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000, 5500, 6000, 6500, 7000, 7500, 8000, 8500, 9000, 9500, 10000, 10500, 11000, 11500, 12000, 12500, 13000, 13500, 14000, 14500, 15000, 15500, 16000, 16500, 17000, 17500, 18000, 18500, 19000, 19500, 20000, 20500, 21000, 21500, 22000, 22500, 23000, 23500, 24000, 24500, 25000, 25500, 26000, 26500, 27000, 27500, 28000, 28500, 29000, 29500, 30000, 30500, 31000, 31500, 32000, 32500, 33000, 33500, 34000, 34500, 35000, 35500, 36000, 36500, 37000, 37500, 38000, 38500, 39000, 39500, 40000, 40500, 41000, 41500, 42000, 42500, 43000, 43500, 44000, 44500, 45000, 45500, 46000, 46500, 47000, 47500, 48000, 48500, 49000, 49500, 50000, 50500, 51000, 51500, 52000, 52500, 53000, 53500, 54000, 54500, 55000, 55500, 56000, 56500, 57000, 57500, 58000, 58500, 59000, 59500, 60000, 60500, 61000, 61500, 62000, 62500, 63000, 63500, 64000, 64500, 65000, 65500, 66000, 66500, 67000, 67500, 68000, 68500, 69000, 69500, 70000, 70500, 71000, 71500, 72000, 72500, 73000, 73500, 74000, 74500, 75000, 75500, 76000, 76500, 77000, 77500, 78000, 78500, 79000, 79500, 80000, 80500, 81000, 81500, 82000, 82500, 83000, 83500, 84000, 84500, 85000, 85500, 86000, 86500, 87000, 87500, 88000, 88500, 89000, 89500, 90000, 90500, 91000, 91500, 92000, 92500, 93000, 93500, 94000, 94500, 95000, 95500, 96000, Error: local variable 'pub_dt' referenced before assignment on pubDateRecord:\n",
      "{'PMCID': 'PMC7668269', 'pubDates': [{'pubdate': ' 2020  Oct 1'}, {'epubdate': ' 2020  Oct 1'}, {'printpubdate': ''}, {'sortdate': '2020/10/01 00:00'}, {'pmclivedate': '2020/11/24'}]}\n",
      "96500, Error: local variable 'pub_dt' referenced before assignment on pubDateRecord:\n",
      "{'PMCID': 'PMC7668271', 'pubDates': [{'pubdate': ' 2020  Jun 1'}, {'epubdate': ' 2020  Jun 1'}, {'printpubdate': ''}, {'sortdate': '2020/06/01 00:00'}, {'pmclivedate': '2020/11/24'}]}\n",
      "Error: local variable 'pub_dt' referenced before assignment on pubDateRecord:\n",
      "{'PMCID': 'PMC7668270', 'pubDates': [{'pubdate': ' 2020  Oct 1'}, {'epubdate': ' 2020  Oct 1'}, {'printpubdate': ''}, {'sortdate': '2020/10/01 00:00'}, {'pmclivedate': '2020/11/24'}]}\n"
     ]
    }
   ],
   "source": [
    "pubDates_df = [None] * len(pubDates_db)  # preallocate\n",
    "\n",
    "print(f\"processing {len(pubDates_db)} dates\")\n",
    "for i,p in enumerate(pubDates_db):\n",
    "    if i % 500 == 0:\n",
    "        print(i, end=', ')\n",
    "    \n",
    "    PMCID = p['PMCID']\n",
    "    pubDate = get_pubDate(PMCID)\n",
    "    pubDates_df[i] = { 'PMCID': PMCID, 'pubDate': pubDate }\n",
    "\n",
    "pubDates_df = pd.DataFrame(pubDates_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pubDates_df.to_csv(join(dataDir, 'processed', 'publicationDates.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PMCID</th>\n",
       "      <th>pubDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PMC7250740</td>\n",
       "      <td>2020-05-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PMC7573915</td>\n",
       "      <td>2020-09-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PMC7454712</td>\n",
       "      <td>2020-08-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PMC7439822</td>\n",
       "      <td>2020-08-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PMC7530335</td>\n",
       "      <td>2020-09-18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        PMCID     pubDate\n",
       "0  PMC7250740  2020-05-27\n",
       "1  PMC7573915  2020-09-18\n",
       "2  PMC7454712  2020-08-12\n",
       "3  PMC7439822  2020-08-20\n",
       "4  PMC7530335  2020-09-18"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pubDates_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the COVID-19 articles retrieved from PubMed\n",
    "\n",
    "Articles retrieved via `getArticles` notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "arts = []\n",
    "with open(join(dataDir, 'articleMetadata.json')) as f:\n",
    "    for i,line in enumerate(f):\n",
    "        thisArt = json.loads(line)\n",
    "        if 'isValid' in thisArt.keys():\n",
    "            arts.append(thisArt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amend/Clean article metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete number of authors and affiliations from the metadata for each article\n",
    "def get_nAuthors(md):\n",
    "    if 'nAuthors' in md.keys():\n",
    "        return md['nAuthors']\n",
    "    if 'authors' not in md.keys():\n",
    "        return 0\n",
    "    if md['authors'] == None:\n",
    "        return 0\n",
    "    return len(md['authors']['authors'])\n",
    "\n",
    "def get_nLocations(md):\n",
    "    if 'nLocations' in md.keys():\n",
    "        return md['nLocations']\n",
    "    if 'authors' not in md.keys():\n",
    "        return 0\n",
    "    if md['authors'] == None:\n",
    "        return 0\n",
    "    return len(md['authors']['affiliations'])\n",
    "\n",
    "def get_pubDate(md):\n",
    "    PMCID = md['PMCID']\n",
    "    match = pubDates_df[pubDates_df['PMCID'] == PMCID]\n",
    "    if match.empty:\n",
    "        return None\n",
    "    return match.iloc[0]['pubDate']\n",
    "    \n",
    "# Clean the location names\n",
    "def cleanLocations(md):\n",
    "    if 'authors' not in md.keys():\n",
    "        return None\n",
    "    if md['authors'] == None:\n",
    "        return None\n",
    "    for loc in md['authors']['affiliations']:\n",
    "        loc['address'] = cleanLocationString(loc['address'])\n",
    "    return md\n",
    "\n",
    "def cleanLocationString(locStr):\n",
    "    \"\"\" Some location names have spaces b e t w e e n every character. This'll fix it \"\"\"\n",
    "    rexp = '(\\S\\s)+\\S(?=\\s{1,})'\n",
    "    matches = [x for x in re.finditer(r'(\\S\\s)+\\S(?=\\s{1,})', locStr)]  # see if there are any matches\n",
    "    if len(matches) == 0:\n",
    "        return locStr\n",
    "\n",
    "    # search and replace once for each match\n",
    "    for i in matches:\n",
    "        thisMatch = re.search(r'(\\S\\s)+\\S(?=\\s{1,})', locStr)   # find the match, along with start and end position from current string\n",
    "        startIdx = thisMatch.start()\n",
    "        endIdx = thisMatch.end()\n",
    "\n",
    "        cleaned = thisMatch.group().replace(' ', '')     # remove the spaces from the match\n",
    "        locStr = locStr[:startIdx] + cleaned + locStr[endIdx:]  # insert the cleaned word back into the original string\n",
    "\n",
    "    # replace any 2 or more whitespaces with just 1 whitespace\n",
    "    locStr = re.sub(r\"\\s{2,}\", \" \", locStr)\n",
    "\n",
    "    return locStr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step takes ~20min to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for art in arts:\n",
    "    art['nAuthors'] = get_nAuthors(art)\n",
    "    art['pubDate'] = get_pubDate(art)\n",
    "    art['nLocations'] = get_nLocations(art)\n",
    "    art = cleanLocations(art)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93593: total articles\n",
      "--------------\n",
      "4461: no Authors\n",
      "7461: no Locations\n",
      "71: no Title\n",
      "3559: no Date\n"
     ]
    }
   ],
   "source": [
    "nArts = len(arts)\n",
    "print('{}: valid articles'.format(nArts))\n",
    "print('--------------')\n",
    "\n",
    "# count articles with missing fields\n",
    "summary = {\n",
    "    'Authors': len([x for x in arts if x['nAuthors'] == 0]),\n",
    "    'Locations': len([x for x in arts if x['nLocations'] == 0]),\n",
    "    'Title': len([x for x in arts if x['title'] is None]),\n",
    "    'Date': len([x for x in arts if x['pubDates'] is None])\n",
    "}\n",
    "\n",
    "for k,v in summary.items():\n",
    "    print('{}: no {}'.format(v,k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update unique locations\n",
    "\n",
    "Use the latest set of articles to update the Unique Locations and GeoCodes databases\n",
    "\n",
    "**Unique Locations** database contains the set of unique address strings found across all affiliations listed in the article metadata. Each address string is associated with a unique location id (`locID`) as well as processed address information and a `geoID` (where applicable) that links the locID to a particular geocode entry in the **cityGeocodes** database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools\n",
    "\n",
    "#### Preprocess Addresses \n",
    "Use the python bindings for the [libpostal library](https://github.com/openvenues/libpostal) to attempt to extract structured address details from the unstructured strings associated with each location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from postal.parser import parse_address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processAddress(addrStr):\n",
    "    \"\"\"parse the given address string. return structured address components\"\"\"\n",
    "    addr = {\n",
    "        'origAddr': addrStr,\n",
    "        'city': None,\n",
    "        'state': None,\n",
    "        'country': None\n",
    "    }\n",
    "    \n",
    "    # parse the address using libpostal\n",
    "    addrParts = parse_address(addrStr)\n",
    "    \n",
    "    # Find the first instance of city, state, or country in parsed address. \n",
    "    # Given that addresses are typically written in descending order of geographic specificity,\n",
    "    # iterate through parts list backwards in case the parser has returned 2 or more instances of\n",
    "    # city or county components. In these cases, the latter instance is usually the one we want. \n",
    "    for p in reversed(addrParts):\n",
    "        if p[1] == 'city':\n",
    "            addr['city'] = p[0]\n",
    "            break\n",
    "    \n",
    "    for p in reversed(addrParts):\n",
    "        if p[1] == 'state':\n",
    "            addr['state'] = p[0]\n",
    "            break\n",
    "            \n",
    "    for p in reversed(addrParts):\n",
    "        if p[1] == 'country':\n",
    "            addr['country'] = p[0]\n",
    "            break\n",
    "            \n",
    "    return addr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geocode Addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy import GoogleV3\n",
    "from geopy.extra.rate_limiter import RateLimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAddressCmpt(rawGeo, cmpt):\n",
    "    \"\"\"\n",
    "    return the specified address component from the raw geo\n",
    "    'cmpt' expected to be one of ['locality', 'country', 'administrative_area_level_1']\n",
    "    \"\"\"\n",
    "    addrParts = rawGeo['address_components']\n",
    "    for p in addrParts:\n",
    "        if cmpt in p['types']:\n",
    "            return p['long_name']\n",
    "    return None\n",
    "\n",
    "def getAddrCmptFromID(geoID, cmpt):\n",
    "    \"\"\"return the desired address component associated with the desired geoID\"\"\"\n",
    "    # find the geo\n",
    "    for r in rawGeo:\n",
    "        if r['place_id'] == geoID:\n",
    "            thisGeo = r\n",
    "            break\n",
    "    \n",
    "    # return the desired field\n",
    "    return getAddressCmpt(thisGeo, cmpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('googleAPI_key.txt', 'r') as f:\n",
    "    apiKey = f.read().rstrip('\\n')\n",
    "    \n",
    "# instantiate Google Geocoder\n",
    "gmap = GoogleV3(api_key=apiKey)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize current location info from article metadata dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this step currently takes ~5min to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322114 total locations\n",
      "\n",
      "270025 unique location strings across all affiliations\n"
     ]
    }
   ],
   "source": [
    "locs = []\n",
    "for art in arts:\n",
    "    if art['nLocations'] > 0:\n",
    "        artLocs = art['authors']['affiliations']\n",
    "        locs = locs + [x['address'] for x in artLocs]\n",
    "\n",
    "# get list of unique locations\n",
    "uniqLocStrs = list(set(locs))\n",
    "\n",
    "print('{} total locations\\n'.format(len(locs)))\n",
    "print('{} unique location strings across all affiliations'.format(len(uniqLocStrs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create list of new addresses not currently in Unique Locations database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10632 new locations not found already in unique locations db\n"
     ]
    }
   ],
   "source": [
    "# Open (or create) the unique locations database\n",
    "uniqLocs_fname = join(dataDir, 'uniqueLocations.csv')\n",
    "if exists(uniqLocs_fname):\n",
    "    uniqLocs_df = pd.read_csv(uniqLocs_fname)\n",
    "else:\n",
    "    uniqLocs_df = pd.DataFrame(columns=['locID', 'origAddr', 'locality', 'admArea_1', 'country', 'geoID'])\n",
    "    \n",
    "\n",
    "# Figure out which locations are already in the database\n",
    "existingLocs = uniqLocs_df['origAddr']\n",
    "newLocs = list(set(uniqLocStrs).difference(set(existingLocs)))\n",
    "\n",
    "# remove the empty string from newLocs (uniqLocs reps this as \"NaN\", thus won't be in the existingLocs list)\n",
    "newLocs.remove('')\n",
    "\n",
    "print('{} new locations not found already in unique locations db'.format(len(newLocs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "newLocs = list(map(lambda x: {'origAddr': x}, newLocs))\n",
    "\n",
    "# assign a unique location ID to each new location, numbering up from however many locations already exist\n",
    "startIdx = uniqLocs_df.shape[0]\n",
    "for i,loc in enumerate(newLocs, start=1):\n",
    "    locID = 'loc{}'.format(str(startIdx + i).zfill(9))\n",
    "    loc['locID'] = locID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get GeoCodes for new addresses\n",
    "\n",
    "Due to the cost of hitting Google's geocode API with hundreds of thousands of location strings, keeping the geocodes at the level of cities-only for now\n",
    "\n",
    "This step will create or update 2 different databases:\n",
    "\n",
    "* **cityGeocodes**:   \n",
    "    a table of geocoded location data. one entry for every unique city\n",
    "    \n",
    "* **geoSearchStrs**: a table linking given search strings to a specific geoID in the cityGeocodes table. Due to how searches are constructed, multiple different search strings may link to the same phyical geo location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking geocodes for 10632 addresses\n",
      "0, 50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 1000, 1050, 1100, 1150, 1200, 1250, 1300, 1350, 1400, 1450, 1500, 1550, 1600, 1650, 1700, 1750, 1800, 1850, 1900, 1950, Non-successful status code 400\n",
      "2000, 2050, 2100, 2150, 2200, 2250, Non-successful status code 400\n",
      "2300, 2350, 2400, 2450, 2500, 2550, 2600, 2650, 2700, 2750, 2800, 2850, 2900, 2950, 3000, 3050, 3100, 3150, 3200, 3250, 3300, 3350, 3400, 3450, 3500, 3550, 3600, 3650, 3700, 3750, 3800, 3850, 3900, 3950, 4000, 4050, 4100, 4150, 4200, 4250, 4300, 4350, 4400, 4450, 4500, 4550, 4600, 4650, 4700, 4750, 4800, 4850, 4900, 4950, 5000, 5050, 5100, 5150, 5200, 5250, 5300, 5350, 5400, 5450, 5500, 5550, 5600, 5650, 5700, 5750, 5800, 5850, 5900, 5950, 6000, 6050, 6100, 6150, 6200, 6250, 6300, 6350, 6400, 6450, 6500, 6550, 6600, 6650, 6700, 6750, 6800, 6850, Non-successful status code 400\n",
      "6900, 6950, 7000, 7050, 7100, 7150, 7200, 7250, 7300, 7350, 7400, 7450, 7500, 7550, 7600, 7650, 7700, 7750, 7800, 7850, 7900, 7950, 8000, 8050, 8100, 8150, 8200, 8250, 8300, 8350, 8400, 8450, 8500, 8550, 8600, 8650, Non-successful status code 400\n",
      "8700, 8750, 8800, 8850, 8900, 8950, 9000, 9050, 9100, 9150, 9200, 9250, 9300, 9350, 9400, 9450, 9500, 9550, 9600, 9650, 9700, 9750, 9800, 9850, 9900, 9950, 10000, 10050, 10100, 10150, 10200, 10250, 10300, 10350, 10400, 10450, 10500, 10550, 10600, \n",
      " Missed 4 locations\n"
     ]
    }
   ],
   "source": [
    "# Open (or create) the city geocodes database\n",
    "geo_fname = join(dataDir, 'cityGeocodes.csv')\n",
    "if exists(geo_fname):\n",
    "    geo_df = pd.read_csv(geo_fname)\n",
    "else:\n",
    "    geo_df = pd.DataFrame(columns=['geoID', 'formattedAddr', 'lat', 'lng', 'locality', 'admArea_1', 'country'])\n",
    "\n",
    "# Open (or create) the geocodes search string's database\n",
    "searchStrs_fname = join(dataDir, 'geoSearchStrs.csv')\n",
    "if exists(searchStrs_fname):\n",
    "    searchStrs_df = pd.read_csv(searchStrs_fname)\n",
    "else:\n",
    "    searchStrs_df = pd.DataFrame(columns=['searchStr', 'geoID'])\n",
    "\n",
    "# Iterate through new locations. If geoID already exists, use it. If not, get geoID and geocode info\n",
    "# Build a location object that can be appended to the unique locations database\n",
    "print('Checking geocodes for {} addresses'.format(len(newLocs)))\n",
    "missedLocs = []\n",
    "for i,loc in enumerate(newLocs):\n",
    "    if i % 50 == 0: print('{}'.format(i), end=', ')  # progress update\n",
    "        \n",
    "    # add the remaining missing fields to loc object, initialize with None\n",
    "    # Note: loc is updated in place\n",
    "    loc['locality'] = np.nan\n",
    "    loc['admArea_1'] = np.nan\n",
    "    loc['country'] = np.nan\n",
    "    loc['geoID'] = np.nan\n",
    "    \n",
    "    ## turn it into an address using libpostal\n",
    "    addr = processAddress(loc['origAddr'])\n",
    "    if addr['city'] is None:\n",
    "        continue              # geocoding at the level of city, so need there to be a \"city\" in the address\n",
    "    \n",
    "    # build a search string based on city, state, country (if each exists)\n",
    "    searchStr = ' '.join([addr[x] for x in ['city', 'state', 'country'] if addr[x] is not None])\n",
    "    \n",
    "    # check if this searchStr already exists in the searchStr database. If so, skip geocoding it again\n",
    "    if searchStr in searchStrs_df['searchStr'].values:\n",
    "        geoID = searchStrs_df[searchStrs_df['searchStr'] == searchStr]['geoID'].item()        \n",
    "        thisGeo = geo_df[geo_df['geoID'] == geoID]\n",
    "        if thisGeo.empty:\n",
    "            continue\n",
    "        for field in ['geoID', 'locality', 'admArea_1', 'country']:\n",
    "            loc[field] = thisGeo[field].item()\n",
    "\n",
    "        continue\n",
    "    \n",
    "    ###### GEOCODE PHASE 1 #####################################################################\n",
    "    # use search string to get geocode info from google \n",
    "    # Parse the 'locality', 'admin_area_level_1', and 'country' from the results, which\n",
    "    # will be used to see if this location already exists in the cityGeocodes database\n",
    "    # (e.g. if this search string refers to a different address in the same city)\n",
    "    time.sleep(.03)       # avoid getting blocked. max 50/s\n",
    "    try:\n",
    "        geo = gmap.geocode(searchStr, exactly_one=True)\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        missedLocs.append(loc)  # save the missed location\n",
    "        continue\n",
    "    \n",
    "    # if unable to geocode this searchstr, add the search to searchStrs db and skip to next loc\n",
    "    if geo is None:\n",
    "        searchStrs_df = searchStrs_df.append({'searchStr': searchStr, 'geoID': None}, ignore_index=True)\n",
    "        continue\n",
    "        \n",
    "    # check if this geo's LOCALITY-ADMINAREA_1-COUNTRY already exists in the cities database\n",
    "    locality = getAddressCmpt(geo.raw, 'locality')\n",
    "    admArea_1 = getAddressCmpt(geo.raw, 'administrative_area_level_1')\n",
    "    country = getAddressCmpt(geo.raw, 'country')\n",
    "    \n",
    "    matching_geo = geo_df[(geo_df['locality'] == locality) & (geo_df['admArea_1'] == admArea_1) & (geo_df['country'] == country)]\n",
    "    if not matching_geo.empty:\n",
    "        matching_geo = matching_geo.iloc[0]\n",
    "            \n",
    "        # get the details for this location from the existing entry in city geocodes\n",
    "        for field in ['locality', 'admArea_1', 'country', 'geoID']:\n",
    "            loc[field] = matching_geo[field]\n",
    "            \n",
    "        # pair this geoID to this search string in the searchStrs db\n",
    "        searchStrs_df = searchStrs_df.append({\n",
    "            'searchStr': searchStr, \n",
    "            'geoID': matching_geo['geoID']}, \n",
    "        ignore_index=True)\n",
    "        \n",
    "        # save searchStr database\n",
    "        searchStrs_df.to_csv(searchStrs_fname, index=False)\n",
    "        \n",
    "        # skip to next location\n",
    "        continue\n",
    "    \n",
    "    ###### GEOCODE PHASE 2 ##################################################################\n",
    "    # since this LOCALITY-ADMINEAREA_1-COUNTRY not already in cities database,  \n",
    "    # submit a new GEOCODE request, using the '<locality> <admin area> <county>' as a search string. \n",
    "    # Parse the response to get the GEOID, LAT, LNG, FORMATTED ADDRESS, LOCALITY, ADM AREA 1, COUNTRY \n",
    "    # of this new location. This will return geocode info specific to this city. Add to the cities database\n",
    "    \n",
    "    # build a new search string based on locality, admin area 1, country (if each exists)\n",
    "    citySearchStr = ' '.join([x for x in [locality, admArea_1, country] if x is not None])\n",
    "    \n",
    "    # geocode\n",
    "    time.sleep(.03)       # avoid getting blocked. max 50/s\n",
    "    try:\n",
    "        geo = gmap.geocode(citySearchStr, exactly_one=True)\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        missedLocs.append(loc)  # save the missed address\n",
    "        continue\n",
    "        \n",
    "    # if unable to geocode this citySearchStr, add the original search to searchStrs db and skip to next loc\n",
    "    if geo is None:\n",
    "        searchStrs_df = searchStrs_df.append({'searchStr': searchStr, 'geoID': None}, ignore_index=True)\n",
    "        continue\n",
    "    \n",
    "    # must match to a locality\n",
    "    locality = getAddressCmpt(geo.raw, 'locality')\n",
    "    if locality is None:\n",
    "        searchStrs_df = searchStrs_df.append({'searchStr': searchStr, 'geoID': None}, ignore_index=True)\n",
    "        continue\n",
    "\n",
    "    # add this entry to the database\n",
    "    geo_obj = {\n",
    "        'geoID': geo.raw['place_id'],\n",
    "        'lat': geo.latitude,\n",
    "        'lng': geo.longitude,\n",
    "        'formattedAddr': geo.raw['formatted_address'],\n",
    "        'locality': locality,\n",
    "        'admArea_1': getAddressCmpt(geo.raw, 'administrative_area_level_1'),\n",
    "        'country': getAddressCmpt(geo.raw, 'country'),\n",
    "    }\n",
    "    \n",
    "    # append this geo obj to the city geocodes db\n",
    "    geo_obj = {k: np.nan if not v else v for k,v in geo_obj.items()}  # convert Nones to NaN\n",
    "    geo_df = geo_df.append(geo_obj, ignore_index=True)\n",
    "    \n",
    "    # update all fields for this location object\n",
    "    for field in ['locality', 'admArea_1', 'country', 'geoID']:\n",
    "        loc[field] = geo_obj[field]\n",
    "    \n",
    "    # pair this geoID with the original search string\n",
    "    searchStrs_df = searchStrs_df.append({\n",
    "        'searchStr': searchStr, \n",
    "        'geoID': geo.raw['place_id']\n",
    "    }, ignore_index=True)\n",
    "    \n",
    "    # save (not efficient, but worth it if checking thousands of locations)\n",
    "    geo_df = geo_df.drop_duplicates(subset='geoID')\n",
    "    geo_df.to_csv(geo_fname, index=False)\n",
    "    searchStrs_df.to_csv(searchStrs_fname, index=False)\n",
    "    raw_fname = join(dataDir, 'rawGeoResponses.json')\n",
    "    with open(raw_fname, 'a', encoding='utf-8') as outfile:\n",
    "        json.dump(geo.raw, outfile, ensure_ascii=False)\n",
    "        outfile.write('\\n')\n",
    "    \n",
    "print('\\n Missed {} locations'.format(len(missedLocs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the newly processed locations to the unique location strings database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqLocs_df = uniqLocs_df.append(newLocs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "uniqLocs_df.to_csv(uniqLocs_fname, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add location IDs to affiliations for all articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLocID(addrStr):\n",
    "    \"\"\" Return the location ID and valid geocode flag based on addr string\"\"\"\n",
    "    if addrStr == '':\n",
    "        loc = uniqLocs_df[uniqLocs_df['origAddr'].isnull()].iloc[0]\n",
    "        return {'locID': loc['locID'], 'hasGeo': False}\n",
    "    \n",
    "    loc = uniqLocs_df[uniqLocs_df['origAddr'] == addrStr]\n",
    "    if loc.empty:\n",
    "        return {'locID': None, 'hasGeo': False}\n",
    "    \n",
    "    # get locID and check for valid geoID\n",
    "    locID = loc['locID'].item()\n",
    "    if pd.isnull(loc['geoID'].item()) or loc['geoID'].item() is None:\n",
    "        return {'locID': locID, 'hasGeo': False}\n",
    "    else:\n",
    "        hasGeo = True\n",
    "        geoID = loc['geoID'].item()\n",
    "        return {'locID': locID, 'hasGeo': True, 'geoID': geoID}\n",
    "    \n",
    "\n",
    "def getArtLocs(art):\n",
    "    \"\"\"process the given article to get all locations for all affiliations\"\"\"\n",
    "    locsWithGeo = 0\n",
    "    for loc in art['authors']['affiliations']:\n",
    "        if 'locID' not in loc.keys():\n",
    "            locInfo = getLocID(loc['address'])\n",
    "            loc['locID'] = locInfo['locID']\n",
    "            loc['hasGeo'] = locInfo['hasGeo']\n",
    "        if loc['hasGeo']:\n",
    "            locsWithGeo += 1\n",
    "\n",
    "    art['nLocsWithGeo'] = locsWithGeo\n",
    "    \n",
    "    return art\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step takes a while to run (~1hr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over all articles to get location for all affiliations\n",
    "for art in arts:\n",
    "    locsWithGeo = 0\n",
    "    for loc in art['authors']['affiliations']:\n",
    "        #if 'locID' not in loc.keys():\n",
    "        locInfo = getLocID(loc['address'])\n",
    "        loc['locID'] = locInfo['locID']\n",
    "        loc['hasGeo'] = locInfo['hasGeo']\n",
    "        if loc['hasGeo']:\n",
    "            loc['geoID'] = locInfo['geoID']\n",
    "            locsWithGeo += 1\n",
    "\n",
    "    art['nLocsWithGeo'] = locsWithGeo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save processed articles back to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(join(dataDir, 'articleMetadata.json'), 'w', encoding='utf-8') as f:\n",
    "    for art in arts:\n",
    "        json.dump(art, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize Articles with Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93593 total articles\n",
      "(68.24%) 63871 w/ at least 2 affiliations\n",
      "(54.03%) 50564 w/ at least 2 geocoded affiliation\n"
     ]
    }
   ],
   "source": [
    "totalArts = len(arts)\n",
    "multiLocs = len([x for x in arts if x['nLocations'] >= 2])\n",
    "multiLocsWithGeo = len([x for x in arts if x['nLocsWithGeo'] >= 2])\n",
    "\n",
    "print('{} total articles'.format(totalArts))\n",
    "print('({}%) {} w/ at least 2 affiliations'.format(np.round(multiLocs/totalArts*100, 2), multiLocs))\n",
    "print('({}%) {} w/ at least 2 geocoded affiliation'.format(np.round(multiLocsWithGeo/totalArts * 100, 2), multiLocsWithGeo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1074979 total geocoded collaborations\n"
     ]
    }
   ],
   "source": [
    "# calculate the total number of collaborations across all articles with geocoded affiliations\n",
    "# but note that this doesn't take into account locations with the same geo on the same paper\n",
    "collabs = 0\n",
    "for art in arts:\n",
    "    if art['nLocsWithGeo'] >=2:\n",
    "        nLocs = art['nLocsWithGeo']\n",
    "        collabs += nLocs * (nLocs-1) / 2  # calculate unique collabs\n",
    "\n",
    "print('{} total geocoded collaborations'.format(int(collabs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spot check the unique locations for accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "testLocs = uniqLocs_df.sample(100)\n",
    "testLocs.to_csv(join(dataDir, 'testLocations.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
