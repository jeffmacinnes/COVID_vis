{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from os.path import join\n",
    "from os.path import exists\n",
    "\n",
    "import json\n",
    "import requests\n",
    "import xmltodict\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir = '../data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the COVID-19 articles retrieved from PubMed\n",
    "\n",
    "Articles retrieved via `getArticles` notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "arts = []\n",
    "with open(join(dataDir, 'articleMetadata.json')) as f:\n",
    "    for i,line in enumerate(f):\n",
    "        thisArt = json.loads(line)\n",
    "        if 'isValid' in thisArt.keys():\n",
    "            arts.append(thisArt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amend/Clean article metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete number of authors and affiliations from the metadata for each article\n",
    "def get_nAuthors(md):\n",
    "    if 'nAuthors' in md.keys():\n",
    "        return md['nAuthors']\n",
    "    if 'authors' not in md.keys():\n",
    "        return 0\n",
    "    if md['authors'] == None:\n",
    "        return 0\n",
    "    return len(md['authors']['authors'])\n",
    "\n",
    "def get_nLocations(md):\n",
    "    if 'nLocations' in md.keys():\n",
    "        return md['nLocations']\n",
    "    if 'authors' not in md.keys():\n",
    "        return 0\n",
    "    if md['authors'] == None:\n",
    "        return 0\n",
    "    return len(md['authors']['affiliations'])\n",
    "\n",
    "# Clean the location names\n",
    "def cleanLocations(md):\n",
    "    if 'authors' not in md.keys():\n",
    "        return None\n",
    "    if md['authors'] == None:\n",
    "        return None\n",
    "    for loc in md['authors']['affiliations']:\n",
    "        loc['address'] = cleanLocationString(loc['address'])\n",
    "    return md\n",
    "\n",
    "def cleanLocationString(locStr):\n",
    "    \"\"\" Some location names have spaces b e t w e e n every character. This'll fix it \"\"\"\n",
    "    rexp = '(\\S\\s)+\\S(?=\\s{1,})'\n",
    "    matches = [x for x in re.finditer(r'(\\S\\s)+\\S(?=\\s{1,})', locStr)]  # see if there are any matches\n",
    "    if len(matches) == 0:\n",
    "        return locStr\n",
    "\n",
    "    # search and replace once for each match\n",
    "    for i in matches:\n",
    "        thisMatch = re.search(r'(\\S\\s)+\\S(?=\\s{1,})', locStr)   # find the match, along with start and end position from current string\n",
    "        startIdx = thisMatch.start()\n",
    "        endIdx = thisMatch.end()\n",
    "\n",
    "        cleaned = thisMatch.group().replace(' ', '')     # remove the spaces from the match\n",
    "        locStr = locStr[:startIdx] + cleaned + locStr[endIdx:]  # insert the cleaned word back into the original string\n",
    "\n",
    "    # replace any 2 or more whitespaces with just 1 whitespace\n",
    "    locStr = re.sub(r\"\\s{2,}\", \" \", locStr)\n",
    "\n",
    "    return locStr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for art in arts:\n",
    "    art['nAuthors'] = get_nAuthors(art)\n",
    "    art['nLocations'] = get_nLocations(art)\n",
    "    art = cleanLocations(art)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88797: total articles\n",
      "--------------\n",
      "4187: no Authors\n",
      "7083: no Locations\n",
      "65: no Title\n",
      "3280: no Date\n"
     ]
    }
   ],
   "source": [
    "nArts = len(arts)\n",
    "print('{}: total articles'.format(nArts))\n",
    "print('--------------')\n",
    "\n",
    "# count articles with missing fields\n",
    "summary = {\n",
    "    'Authors': len([x for x in arts if x['nAuthors'] == 0]),\n",
    "    'Locations': len([x for x in arts if x['nLocations'] == 0]),\n",
    "    'Title': len([x for x in arts if x['title'] is None]),\n",
    "    'Date': len([x for x in arts if x['pubDates'] is None])\n",
    "}\n",
    "\n",
    "for k,v in summary.items():\n",
    "    print('{}: no {}'.format(v,k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update unique locations\n",
    "\n",
    "Use the latest set of articles to update the Unique Locations and GeoCodes databases\n",
    "\n",
    "**Unique Locations** database contains the set of unique address strings found across all affiliations listed in the article metadata. Each address string is associated with a unique location id (`locID`) as well as processed address information and a `geoID` (where applicable) that links the locID to a particular geocode entry in the **cityGeocodes** database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools\n",
    "\n",
    "#### Preprocess Addresses \n",
    "Use the python bindings for the [libpostal library](https://github.com/openvenues/libpostal) to attempt to extract structured address details from the unstructured strings associated with each location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from postal.parser import parse_address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processAddress(addrStr):\n",
    "    \"\"\"parse the given address string. return structured address components\"\"\"\n",
    "    addr = {\n",
    "        'origAddr': addrStr,\n",
    "        'city': None,\n",
    "        'state': None,\n",
    "        'country': None\n",
    "    }\n",
    "    \n",
    "    # parse the address using libpostal\n",
    "    addrParts = parse_address(addrStr)\n",
    "    \n",
    "    # Find the first instance of city, state, or country in parsed address. \n",
    "    # Given that addresses are typically written in descending order of geographic specificity,\n",
    "    # iterate through parts list backwards in case the parser has returned 2 or more instances of\n",
    "    # city or county components. In these cases, the latter instance is usually the one we want. \n",
    "    for p in reversed(addrParts):\n",
    "        if p[1] == 'city':\n",
    "            addr['city'] = p[0]\n",
    "            break\n",
    "    \n",
    "    for p in reversed(addrParts):\n",
    "        if p[1] == 'state':\n",
    "            addr['state'] = p[0]\n",
    "            break\n",
    "            \n",
    "    for p in reversed(addrParts):\n",
    "        if p[1] == 'country':\n",
    "            addr['country'] = p[0]\n",
    "            break\n",
    "            \n",
    "    return addr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geocode Addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy import GoogleV3\n",
    "from geopy.extra.rate_limiter import RateLimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAddressCmpt(rawGeo, cmpt):\n",
    "    \"\"\"\n",
    "    return the specified address component from the raw geo\n",
    "    'cmpt' expected to be one of ['locality', 'country', 'administrative_area_level_1']\n",
    "    \"\"\"\n",
    "    addrParts = rawGeo['address_components']\n",
    "    for p in addrParts:\n",
    "        if cmpt in p['types']:\n",
    "            return p['long_name']\n",
    "    return None\n",
    "\n",
    "def getAddrCmptFromID(geoID, cmpt):\n",
    "    \"\"\"return the desired address component associated with the desired geoID\"\"\"\n",
    "    # find the geo\n",
    "    for r in rawGeo:\n",
    "        if r['place_id'] == geoID:\n",
    "            thisGeo = r\n",
    "            break\n",
    "    \n",
    "    # return the desired field\n",
    "    return getAddressCmpt(thisGeo, cmpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('googleAPI_key.txt', 'r') as f:\n",
    "    apiKey = f.read().rstrip('\\n')\n",
    "    \n",
    "# instantiate Google Geocoder\n",
    "gmap = GoogleV3(api_key=apiKey)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize current location info from article metadata dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this step currently takes ~5min to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303288 total locations\n",
      "\n",
      "254887 unique location strings across all affiliations\n"
     ]
    }
   ],
   "source": [
    "locs = []\n",
    "for art in arts:\n",
    "    if art['nLocations'] > 0:\n",
    "        artLocs = art['authors']['affiliations']\n",
    "        locs = locs + [x['address'] for x in artLocs]\n",
    "\n",
    "# get list of unique locations\n",
    "uniqLocStrs = list(set(locs))\n",
    "\n",
    "print('{} total locations\\n'.format(len(locs)))\n",
    "print('{} unique location strings across all affiliations'.format(len(uniqLocStrs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create list of new addresses not currently in Unique Locations database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24254 new locations not found already in unique locations db\n"
     ]
    }
   ],
   "source": [
    "# Open (or create) the unique locations database\n",
    "uniqLocs_fname = join(dataDir, 'uniqueLocations.csv')\n",
    "if exists(uniqLocs_fname):\n",
    "    uniqLocs_df = pd.read_csv(uniqLocs_fname)\n",
    "else:\n",
    "    uniqLocs_df = pd.DataFrame(columns=['locID', 'origAddr', 'locality', 'admArea_1', 'country', 'geoID'])\n",
    "    \n",
    "\n",
    "# Figure out which locations are already in the database\n",
    "existingLocs = uniqLocs_df['origAddr']\n",
    "newLocs = list(set(uniqLocStrs).difference(set(existingLocs)))\n",
    "\n",
    "# remove the empty string from newLocs (uniqLocs reps this as \"NaN\", thus won't be in the existingLocs list)\n",
    "newLocs.remove('')\n",
    "\n",
    "print('{} new locations not found already in unique locations db'.format(len(newLocs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "newLocs = list(map(lambda x: {'origAddr': x}, newLocs))\n",
    "\n",
    "# assign a unique location ID to each new location, numbering up from however many locations already exist\n",
    "startIdx = uniqLocs_df.shape[0]\n",
    "for i,loc in enumerate(newLocs, start=1):\n",
    "    locID = 'loc{}'.format(str(startIdx + i).zfill(9))\n",
    "    loc['locID'] = locID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get GeoCodes for new addresses\n",
    "\n",
    "Due to the cost of hitting Google's geocode API with hundreds of thousands of location strings, keeping the geocodes at the level of cities-only for now\n",
    "\n",
    "This step will create or update 2 different databases:\n",
    "\n",
    "* **cityGeocodes**:   \n",
    "    a table of geocoded location data. one entry for every unique city\n",
    "    \n",
    "* **geoSearchStrs**: a table linking given search strings to a specific geoID in the cityGeocodes table. Due to how searches are constructed, multiple different search strings may link to the same phyical geo location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking geocodes for 24254 addresses\n",
      "0, 50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 1000, 1050, 1100, 1150, 1200, 1250, 1300, 1350, 1400, 1450, 1500, 1550, 1600, 1650, 1700, 1750, 1800, 1850, 1900, 1950, 2000, 2050, 2100, 2150, 2200, 2250, 2300, 2350, 2400, 2450, 2500, 2550, 2600, 2650, 2700, 2750, 2800, 2850, 2900, 2950, 3000, 3050, 3100, 3150, 3200, 3250, 3300, 3350, 3400, 3450, 3500, 3550, 3600, 3650, 3700, 3750, 3800, 3850, 3900, 3950, 4000, 4050, 4100, 4150, 4200, 4250, 4300, 4350, 4400, 4450, 4500, 4550, 4600, 4650, 4700, 4750, 4800, 4850, 4900, 4950, 5000, 5050, 5100, 5150, 5200, 5250, 5300, 5350, 5400, 5450, 5500, 5550, 5600, 5650, 5700, 5750, 5800, 5850, 5900, 5950, 6000, 6050, 6100, 6150, 6200, 6250, 6300, 6350, 6400, 6450, 6500, 6550, 6600, 6650, 6700, 6750, 6800, 6850, 6900, 6950, 7000, 7050, 7100, 7150, 7200, 7250, 7300, 7350, 7400, 7450, 7500, 7550, 7600, 7650, 7700, 7750, 7800, 7850, 7900, 7950, 8000, 8050, 8100, 8150, 8200, 8250, 8300, 8350, 8400, 8450, 8500, 8550, 8600, 8650, 8700, 8750, 8800, 8850, 8900, 8950, 9000, 9050, 9100, 9150, 9200, 9250, 9300, 9350, 9400, 9450, 9500, 9550, 9600, 9650, 9700, 9750, 9800, 9850, 9900, 9950, 10000, 10050, 10100, 10150, 10200, 10250, 10300, 10350, 10400, 10450, 10500, 10550, 10600, 10650, 10700, 10750, 10800, 10850, 10900, 10950, 11000, 11050, 11100, 11150, 11200, 11250, 11300, 11350, 11400, 11450, 11500, 11550, 11600, 11650, 11700, 11750, 11800, 11850, 11900, 11950, 12000, 12050, 12100, 12150, 12200, 12250, 12300, 12350, 12400, 12450, 12500, 12550, 12600, 12650, 12700, 12750, 12800, 12850, 12900, 12950, 13000, 13050, 13100, 13150, 13200, 13250, 13300, 13350, 13400, 13450, 13500, 13550, 13600, 13650, 13700, 13750, 13800, 13850, 13900, 13950, 14000, 14050, 14100, 14150, 14200, 14250, 14300, 14350, 14400, 14450, 14500, 14550, 14600, 14650, 14700, 14750, 14800, 14850, 14900, 14950, 15000, 15050, 15100, 15150, 15200, 15250, 15300, 15350, 15400, 15450, 15500, Non-successful status code 400\n",
      "15550, 15600, 15650, 15700, 15750, 15800, 15850, 15900, 15950, 16000, 16050, 16100, 16150, 16200, 16250, 16300, 16350, 16400, 16450, 16500, 16550, 16600, 16650, 16700, 16750, 16800, 16850, 16900, 16950, 17000, 17050, 17100, 17150, 17200, 17250, 17300, 17350, 17400, 17450, 17500, 17550, 17600, 17650, 17700, 17750, 17800, 17850, 17900, 17950, 18000, 18050, 18100, 18150, 18200, 18250, 18300, 18350, 18400, 18450, 18500, 18550, 18600, 18650, 18700, 18750, 18800, 18850, 18900, 18950, 19000, 19050, 19100, 19150, 19200, 19250, 19300, 19350, 19400, 19450, 19500, 19550, 19600, 19650, 19700, 19750, 19800, 19850, 19900, 19950, 20000, 20050, 20100, 20150, 20200, 20250, 20300, 20350, 20400, 20450, 20500, 20550, 20600, 20650, 20700, 20750, 20800, 20850, 20900, 20950, 21000, 21050, 21100, 21150, 21200, 21250, 21300, 21350, 21400, 21450, 21500, 21550, 21600, 21650, 21700, 21750, 21800, 21850, 21900, 21950, 22000, 22050, 22100, 22150, 22200, 22250, 22300, 22350, 22400, 22450, 22500, 22550, 22600, 22650, 22700, 22750, 22800, 22850, 22900, 22950, 23000, 23050, 23100, 23150, 23200, 23250, 23300, 23350, 23400, 23450, 23500, 23550, 23600, 23650, 23700, 23750, 23800, 23850, 23900, 23950, 24000, 24050, 24100, 24150, 24200, 24250, \n",
      " Missed 1 locations\n"
     ]
    }
   ],
   "source": [
    "# Open (or create) the city geocodes database\n",
    "geo_fname = join(dataDir, 'cityGeocodes.csv')\n",
    "if exists(geo_fname):\n",
    "    geo_df = pd.read_csv(geo_fname)\n",
    "else:\n",
    "    geo_df = pd.DataFrame(columns=['geoID', 'formattedAddr', 'lat', 'lng', 'locality', 'admArea_1', 'country'])\n",
    "\n",
    "# Open (or create) the geocodes search string's database\n",
    "searchStrs_fname = join(dataDir, 'geoSearchStrs.csv')\n",
    "if exists(searchStrs_fname):\n",
    "    searchStrs_df = pd.read_csv(searchStrs_fname)\n",
    "else:\n",
    "    searchStrs_df = pd.DataFrame(columns=['searchStr', 'geoID'])\n",
    "\n",
    "# Iterate through new locations. If geoID already exists, use it. If not, get geoID and geocode info\n",
    "# Build a location object that can be appended to the unique locations database\n",
    "print('Checking geocodes for {} addresses'.format(len(newLocs)))\n",
    "missedLocs = []\n",
    "for i,loc in enumerate(newLocs):\n",
    "    if i % 50 == 0: print('{}'.format(i), end=', ')  # progress update\n",
    "        \n",
    "    # add the remaining missing fields to loc object, initialize with None\n",
    "    # Note: loc is updated in place\n",
    "    loc['locality'] = np.nan\n",
    "    loc['admArea_1'] = np.nan\n",
    "    loc['country'] = np.nan\n",
    "    loc['geoID'] = np.nan\n",
    "    \n",
    "    ## turn it into an address using libpostal\n",
    "    addr = processAddress(loc['origAddr'])\n",
    "    if addr['city'] is None:\n",
    "        continue              # geocoding at the level of city, so need there to be a \"city\" in the address\n",
    "    \n",
    "    # build a search string based on city, state, country (if each exists)\n",
    "    searchStr = ' '.join([addr[x] for x in ['city', 'state', 'country'] if addr[x] is not None])\n",
    "    \n",
    "    # check if this searchStr already exists in the searchStr database. If so, skip geocoding it again\n",
    "    if searchStr in searchStrs_df['searchStr'].values:\n",
    "        geoID = searchStrs_df[searchStrs_df['searchStr'] == searchStr]['geoID'].item()        \n",
    "        thisGeo = geo_df[geo_df['geoID'] == geoID]\n",
    "        if thisGeo.empty:\n",
    "            continue\n",
    "        for field in ['geoID', 'locality', 'admArea_1', 'country']:\n",
    "            loc[field] = thisGeo[field].item()\n",
    "\n",
    "        continue\n",
    "    \n",
    "    ###### GEOCODE PHASE 1 #####################################################################\n",
    "    # use search string to get geocode info from google \n",
    "    # Parse the 'locality', 'admin_area_level_1', and 'country' from the results, which\n",
    "    # will be used to see if this location already exists in the cityGeocodes database\n",
    "    # (e.g. if this search string refers to a different address in the same city)\n",
    "    time.sleep(.03)       # avoid getting blocked. max 50/s\n",
    "    try:\n",
    "        geo = gmap.geocode(searchStr, exactly_one=True)\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        missedLocs.append(loc)  # save the missed location\n",
    "        continue\n",
    "    \n",
    "    # if unable to geocode this searchstr, add the search to searchStrs db and skip to next loc\n",
    "    if geo is None:\n",
    "        searchStrs_df = searchStrs_df.append({'searchStr': searchStr, 'geoID': None}, ignore_index=True)\n",
    "        continue\n",
    "        \n",
    "    # check if this geo's LOCALITY-ADMINAREA_1-COUNTRY already exists in the cities database\n",
    "    locality = getAddressCmpt(geo.raw, 'locality')\n",
    "    admArea_1 = getAddressCmpt(geo.raw, 'administrative_area_level_1')\n",
    "    country = getAddressCmpt(geo.raw, 'country')\n",
    "    \n",
    "    matching_geo = geo_df[(geo_df['locality'] == locality) & (geo_df['admArea_1'] == admArea_1) & (geo_df['country'] == country)]\n",
    "    if not matching_geo.empty:\n",
    "        matching_geo = matching_geo.iloc[0]\n",
    "            \n",
    "        # get the details for this location from the existing entry in city geocodes\n",
    "        for field in ['locality', 'admArea_1', 'country', 'geoID']:\n",
    "            loc[field] = matching_geo[field]\n",
    "            \n",
    "        # pair this geoID to this search string in the searchStrs db\n",
    "        searchStrs_df = searchStrs_df.append({\n",
    "            'searchStr': searchStr, \n",
    "            'geoID': matching_geo['geoID']}, \n",
    "        ignore_index=True)\n",
    "        \n",
    "        # save searchStr database\n",
    "        searchStrs_df.to_csv(searchStrs_fname, index=False)\n",
    "        \n",
    "        # skip to next location\n",
    "        continue\n",
    "    \n",
    "    ###### GEOCODE PHASE 2 ##################################################################\n",
    "    # since this LOCALITY-ADMINEAREA_1-COUNTRY not already in cities database,  \n",
    "    # submit a new GEOCODE request, using the '<locality> <admin area> <county>' as a search string. \n",
    "    # Parse the response to get the GEOID, LAT, LNG, FORMATTED ADDRESS, LOCALITY, ADM AREA 1, COUNTRY \n",
    "    # of this new location. This will return geocode info specific to this city. Add to the cities database\n",
    "    \n",
    "    # build a new search string based on locality, admin area 1, country (if each exists)\n",
    "    citySearchStr = ' '.join([x for x in [locality, admArea_1, country] if x is not None])\n",
    "    \n",
    "    # geocode\n",
    "    time.sleep(.03)       # avoid getting blocked. max 50/s\n",
    "    try:\n",
    "        geo = gmap.geocode(citySearchStr, exactly_one=True)\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        missedLocs.append(loc)  # save the missed address\n",
    "        continue\n",
    "        \n",
    "    # if unable to geocode this citySearchStr, add the original search to searchStrs db and skip to next loc\n",
    "    if geo is None:\n",
    "        searchStrs_df = searchStrs_df.append({'searchStr': searchStr, 'geoID': None}, ignore_index=True)\n",
    "        continue\n",
    "    \n",
    "    # must match to a locality\n",
    "    locality = getAddressCmpt(geo.raw, 'locality')\n",
    "    if locality is None:\n",
    "        searchStrs_df = searchStrs_df.append({'searchStr': searchStr, 'geoID': None}, ignore_index=True)\n",
    "        continue\n",
    "\n",
    "    # add this entry to the database\n",
    "    geo_obj = {\n",
    "        'geoID': geo.raw['place_id'],\n",
    "        'lat': geo.latitude,\n",
    "        'lng': geo.longitude,\n",
    "        'formattedAddr': geo.raw['formatted_address'],\n",
    "        'locality': locality,\n",
    "        'admArea_1': getAddressCmpt(geo.raw, 'administrative_area_level_1'),\n",
    "        'country': getAddressCmpt(geo.raw, 'country'),\n",
    "    }\n",
    "    \n",
    "    # append this geo obj to the city geocodes db\n",
    "    geo_obj = {k: np.nan if not v else v for k,v in geo_obj.items()}  # convert Nones to NaN\n",
    "    geo_df = geo_df.append(geo_obj, ignore_index=True)\n",
    "    \n",
    "    # update all fields for this location object\n",
    "    for field in ['locality', 'admArea_1', 'country', 'geoID']:\n",
    "        loc[field] = geo_obj[field]\n",
    "    \n",
    "    # pair this geoID with the original search string\n",
    "    searchStrs_df = searchStrs_df.append({\n",
    "        'searchStr': searchStr, \n",
    "        'geoID': geo.raw['place_id']\n",
    "    }, ignore_index=True)\n",
    "    \n",
    "    # save (not efficient, but worth it if checking thousands of locations)\n",
    "    geo_df = geo_df.drop_duplicates(subset='geoID')\n",
    "    geo_df.to_csv(geo_fname, index=False)\n",
    "    searchStrs_df.to_csv(searchStrs_fname, index=False)\n",
    "    raw_fname = join(dataDir, 'rawGeoResponses.json')\n",
    "    with open(raw_fname, 'a', encoding='utf-8') as outfile:\n",
    "        json.dump(geo.raw, outfile, ensure_ascii=False)\n",
    "        outfile.write('\\n')\n",
    "    \n",
    "print('\\n Missed {} locations'.format(len(missedLocs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the newly processed locations to the unique location strings database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqLocs_df = uniqLocs_df.append(newLocs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "uniqLocs_df.to_csv(uniqLocs_fname, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add location IDs to affiliations for all articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLocID(addrStr):\n",
    "    \"\"\" Return the location ID and valid geocode flag based on addr string\"\"\"\n",
    "    if addrStr == '':\n",
    "        loc = uniqLocs_df[uniqLocs_df['origAddr'].isnull()].iloc[0]\n",
    "        return {'locID': loc['locID'], 'hasGeo': False}\n",
    "    \n",
    "    loc = uniqLocs_df[uniqLocs_df['origAddr'] == addrStr]\n",
    "    if loc.empty:\n",
    "        return {'locID': None, 'hasGeo': False}\n",
    "    \n",
    "    # get locID and check for valid geoID\n",
    "    locID = loc['locID'].item()\n",
    "    if pd.isnull(loc['geoID'].item()) or loc['geoID'].item() is None:\n",
    "        return {'locID': locID, 'hasGeo': False}\n",
    "    else:\n",
    "        hasGeo = True\n",
    "        geoID = loc['geoID'].item()\n",
    "        return {'locID': locID, 'hasGeo': True, 'geoID': geoID}\n",
    "    \n",
    "\n",
    "def getArtLocs(art):\n",
    "    \"\"\"process the given article to get all locations for all affiliations\"\"\"\n",
    "    locsWithGeo = 0\n",
    "    for loc in art['authors']['affiliations']:\n",
    "        if 'locID' not in loc.keys():\n",
    "            locInfo = getLocID(loc['address'])\n",
    "            loc['locID'] = locInfo['locID']\n",
    "            loc['hasGeo'] = locInfo['hasGeo']\n",
    "        if loc['hasGeo']:\n",
    "            locsWithGeo += 1\n",
    "\n",
    "    art['nLocsWithGeo'] = locsWithGeo\n",
    "    \n",
    "    return art\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step takes a while to run (~1hr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over all articles to get location for all affiliations\n",
    "for art in arts:\n",
    "    locsWithGeo = 0\n",
    "    for loc in art['authors']['affiliations']:\n",
    "        #if 'locID' not in loc.keys():\n",
    "        locInfo = getLocID(loc['address'])\n",
    "        loc['locID'] = locInfo['locID']\n",
    "        loc['hasGeo'] = locInfo['hasGeo']\n",
    "        if loc['hasGeo']:\n",
    "            loc['geoID'] = locInfo['geoID']\n",
    "            locsWithGeo += 1\n",
    "\n",
    "    art['nLocsWithGeo'] = locsWithGeo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save processed articles back to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(join(dataDir, 'articleMetadata.json'), 'w', encoding='utf-8') as f:\n",
    "    for art in arts:\n",
    "        json.dump(art, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize Articles with Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88797 total articles\n",
      "(68.08%) 60453 w/ at least 2 affiliations\n",
      "(53.35%) 47371 w/ at least 2 geocoded affiliation\n"
     ]
    }
   ],
   "source": [
    "totalArts = len(arts)\n",
    "multiLocs = len([x for x in arts if x['nLocations'] >= 2])\n",
    "multiLocsWithGeo = len([x for x in arts if x['nLocsWithGeo'] >= 2])\n",
    "\n",
    "print('{} total articles'.format(totalArts))\n",
    "print('({}%) {} w/ at least 2 affiliations'.format(np.round(multiLocs/totalArts*100, 2), multiLocs))\n",
    "print('({}%) {} w/ at least 2 geocoded affiliation'.format(np.round(multiLocsWithGeo/totalArts * 100, 2), multiLocsWithGeo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1005369 total geocoded collaborations\n"
     ]
    }
   ],
   "source": [
    "# calculate the total number of collaborations across all articles with geocoded affiliations\n",
    "collabs = 0\n",
    "for art in arts:\n",
    "    if art['nLocsWithGeo'] >=2:\n",
    "        nLocs = art['nLocsWithGeo']\n",
    "        collabs += nLocs * (nLocs-1) / 2  # calculate unique collabs\n",
    "\n",
    "print('{} total geocoded collaborations'.format(int(collabs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spot check the unique locations for accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "testLocs = uniqLocs_df.sample(100)\n",
    "testLocs.to_csv(join(dataDir, 'testLocations.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
